# ğŸš€ Agent Template ä½¿ç”¨æŒ‡å—

## ğŸ“‹ ç›®éŒ„

1. [å¿«é€Ÿé–‹å§‹](#å¿«é€Ÿé–‹å§‹)
2. [åŸºç¤æ¦‚å¿µ](#åŸºç¤æ¦‚å¿µ)
3. [é€²éšä½¿ç”¨](#é€²éšä½¿ç”¨)
4. [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)
5. [æœ€ä½³å¯¦è¸](#æœ€ä½³å¯¦è¸)

---

## ğŸš€ å¿«é€Ÿé–‹å§‹

### 1. ç’°å¢ƒæº–å‚™

```bash
# 1. å…‹éš†é …ç›®
git clone <your-repo-url>
cd agent-template

# 2. é€²å…¥æºç¢¼ç›®éŒ„
cd src

# 3. å®‰è£ä¾è³´
uv sync  # æ¨è–¦
# æˆ–è€…
pip install -r requirements.txt

# 4. é…ç½®ç’°å¢ƒ
cp .env.example .env
# ç·¨è¼¯ .env æ–‡ä»¶ï¼Œæ·»åŠ æ‚¨çš„ API é‡‘é‘°
```

### 2. ç¬¬ä¸€æ¬¡é‹è¡Œ

```bash
# ç¢ºä¿åœ¨ src ç›®éŒ„ä¸‹
cd src

# é‹è¡Œæ¸¬è©¦ç¯„ä¾‹
python test.py
```

### 3. åŸºæœ¬ä½¿ç”¨æ¨¡å¼

ä»¥ä¸‹æ˜¯ä¸‰ç¨®ä¸»è¦çš„ä½¿ç”¨æ¨¡å¼ï¼š

#### æ¨¡å¼ 1: ç°¡å–® Agent

```python
import asyncio
from agent import AgentFactory, LLM_Provider

async def simple_agent():
    # åˆå§‹åŒ– LLM
    llm = LLM_Provider(model="qwen3:0.6b", provider="ollama")

    # å‰µå»ºå·¥å» 
    factory = AgentFactory()

    # å‰µå»º Agent
    agent = factory.create_agent(
        name="åŠ©ç†",
        description="é€šç”¨åŠ©ç†",
        system_prompt="ä½ æ˜¯ä¸€å€‹æœ‰ç”¨çš„AIåŠ©ç†",
        model=llm.model,
        tools=[],
        max_iterations=5
    )

    # ä½¿ç”¨
    await agent.initialize()
    response = await agent.process_message("ä½ å¥½")
    print(response)

asyncio.run(simple_agent())
```

#### æ¨¡å¼ 2: å…·å‚™å·¥å…·çš„ Agent

```python
from agent.tools import ToolManager, MCPClientService
import json

async def tool_enabled_agent():
    # è¼‰å…¥å·¥å…·é…ç½®
    with open("mcp_config.json", 'r') as f:
        mcp_config = json.load(f)

    # åˆå§‹åŒ–å·¥å…·
    tool_manager = ToolManager()
    mcp_client = MCPClientService(mcp_config["mcpServers"], tool_manager)

    # å‰µå»ºå…·å‚™å·¥å…·çš„ Agent
    factory = AgentFactory(tool_manager=tool_manager)
    agent = factory.create_agent(
        name="å·¥å…·å°ˆå®¶",
        description="èƒ½ä½¿ç”¨å·¥å…·çš„åŠ©ç†",
        system_prompt="ä½ å¯ä»¥ä½¿ç”¨å·¥å…·ä¾†å”åŠ©ç”¨æˆ¶",
        model=llm.model,
        tools=tool_manager.get_tool_names(),  # ä½¿ç”¨æ‰€æœ‰å·¥å…·
        max_iterations=10
    )

    await agent.initialize()
    response = await agent.process_message("å¹«æˆ‘æœç´¢æœ€æ–°çš„AIæ–°è")
    print(response)
```

#### æ¨¡å¼ 3: å¤š Agent å”ä½œ

```python
async def multi_agent_collaboration():
    factory = AgentFactory()

    # å‰µå»ºå°ˆæ¥­åœ˜éšŠ
    team_config = {
        "researcher": {
            "name": "ç ”ç©¶å“¡",
            "description": "å°ˆæ¥­ç ”ç©¶äººå“¡",
            "system_prompt": "ä½ æ˜¯å°ˆæ¥­ç ”ç©¶å“¡ï¼Œè² è²¬è³‡æ–™è’é›†å’Œåˆ†æ",
            "tools": ["web_search"],
            "max_iterations": 15
        },
        "writer": {
            "name": "å¯«æ‰‹",
            "description": "å…§å®¹å‰µä½œå°ˆå®¶",
            "system_prompt": "ä½ æ˜¯å°ˆæ¥­å¯«æ‰‹ï¼Œè² è²¬å…§å®¹å‰µä½œå’Œç·¨è¼¯",
            "tools": ["grammar_checker"],
            "max_iterations": 10
        }
    }

    team = factory.create_multi_agent_team(team_config, llm.model)

    # å”ä½œä»»å‹™
    topic = "AIåœ¨æ•™è‚²ä¸­çš„æ‡‰ç”¨"

    # ç¬¬ä¸€éšæ®µï¼šç ”ç©¶
    research_result = await team["researcher"].process_message(f"ç ”ç©¶{topic}")

    # ç¬¬äºŒéšæ®µï¼šå¯«ä½œ
    final_article = await team["writer"].process_message(
        f"åŸºæ–¼ä»¥ä¸‹ç ”ç©¶å¯«ä¸€ç¯‡æ–‡ç« ï¼š{research_result}"
    )

    return final_article
```

---

## ğŸ§  åŸºç¤æ¦‚å¿µ

### Agent æ¶æ§‹

æœ¬æ¡†æ¶åŸºæ–¼ **ReAct** (Reasoning + Acting) æ¶æ§‹ï¼š

```
æ€è€ƒ (Think) â†’ è¡Œå‹• (Act) â†’ è§€å¯Ÿ (Observe) â†’ åæ€ (Reflect)
```

### æ ¸å¿ƒçµ„ä»¶

1. **LLM_Provider**: çµ±ä¸€çš„èªè¨€æ¨¡å‹æ¥å£
2. **AgentFactory**: Agent å‰µå»ºå’Œç®¡ç†
3. **ToolManager**: å·¥å…·çµ±ä¸€ç®¡ç†
4. **MCPClientService**: MCP å”è­°å·¥å…·æ•´åˆ

### æ•¸æ“šæµ

```
ç”¨æˆ¶è¼¸å…¥ â†’ Agent â†’ LLM â†’ å·¥å…·èª¿ç”¨ â†’ çµæœæ•´åˆ â†’ å›æ‡‰ç”¨æˆ¶
```

---

## ğŸ”§ é€²éšä½¿ç”¨

### è‡ªå®šç¾©å·¥å…·

```python
from langchain_core.tools import BaseTool

class CustomTool(BaseTool):
    name: str = "custom_calculator"
    description: str = "åŸ·è¡Œæ•¸å­¸è¨ˆç®—"

    def _run(self, expression: str) -> str:
        try:
            result = eval(expression)  # æ³¨æ„ï¼šç”Ÿç”¢ç’°å¢ƒéœ€è¦å®‰å…¨è™•ç†
            return f"è¨ˆç®—çµæœ: {result}"
        except Exception as e:
            return f"è¨ˆç®—éŒ¯èª¤: {e}"

# è¨»å†Šå·¥å…·
tool_manager = ToolManager()
tool_manager.register_tool(CustomTool(), category="math")
```

### é…ç½®ç®¡ç†

```python
from agent.config import ConfigManager

# è¼‰å…¥é…ç½®
config_manager = ConfigManager()
config = config_manager.load_config("config.json")

# ä½¿ç”¨é…ç½®å‰µå»ºçµ„ä»¶
llm = LLM_Provider(
    model=config.model.model,
    provider=config.model.provider,
    api_key=config.model.api_key
)
```

---

## ğŸ› æ•…éšœæ’é™¤

### å¸¸è¦‹å•é¡Œ

#### 1. æ¨¡çµ„å°å…¥éŒ¯èª¤

```bash
# ç¢ºä¿åœ¨ src ç›®éŒ„ä¸‹åŸ·è¡Œ
cd src
python test.py
```

#### 2. API é‡‘é‘°é…ç½®éŒ¯èª¤

```python
# æª¢æŸ¥ç’°å¢ƒè®Šæ•¸
import os
print("OpenAI:", "âœ“" if os.getenv("OPENAI_API_KEY") else "âœ—")
print("Anthropic:", "âœ“" if os.getenv("ANTHROPIC_API_KEY") else "âœ—")
```

#### 3. Ollama é€£æ¥å¤±æ•—

```bash
# æª¢æŸ¥ Ollama æ˜¯å¦é‹è¡Œ
curl http://localhost:11434/api/tags

# æ‹‰å–æ¨¡å‹
ollama pull qwen3:0.6b
```

#### 4. MCP å·¥å…·åˆå§‹åŒ–å¤±æ•—

```python
# æª¢æŸ¥ MCP é…ç½®
import json
with open("mcp_config.json", 'r') as f:
    config = json.load(f)
    print("é…ç½®çš„æœå‹™å™¨:", list(config["mcpServers"].keys()))
```

### é™¤éŒ¯æŠ€å·§

```python
import logging

# å•Ÿç”¨è©³ç´°æ—¥èªŒ
logging.basicConfig(level=logging.DEBUG)

# æª¢æŸ¥ Agent ç‹€æ…‹
print(f"Agent åç¨±: {agent.name}")
print(f"å·¥å…·æ•¸é‡: {len(agent.get_available_tools())}")
print(f"å°è©±æ­·å²: {len(agent.get_conversation_history())}")
```

---

## ğŸ† æœ€ä½³å¯¦è¸

### 1. Agent è¨­è¨ˆåŸå‰‡

- **è·è²¬å–®ä¸€**: æ¯å€‹ Agent å°ˆæ³¨æ–¼ç‰¹å®šé ˜åŸŸ
- **æç¤ºæ˜ç¢º**: æ’°å¯«æ¸…æ™°å…·é«”çš„ç³»çµ±æç¤º
- **å·¥å…·é©é…**: é¸æ“‡èˆ‡ä»»å‹™ç›¸é—œçš„å·¥å…·

### 2. æ•ˆèƒ½å„ªåŒ–

```python
# ç”Ÿç”¢ç’°å¢ƒé…ç½®
agent = factory.create_agent(
    name="ç”Ÿç”¢åŠ©ç†",
    description="ç”Ÿç”¢ç’°å¢ƒå„ªåŒ–çš„åŠ©ç†",
    system_prompt="ç°¡æ½”å°ˆæ¥­çš„ç³»çµ±æç¤º",
    model=llm.model,
    tools=["essential_tool"],  # åªä½¿ç”¨å¿…è¦å·¥å…·
    max_iterations=5,  # é™åˆ¶è¿­ä»£æ¬¡æ•¸
    temperature=0.3  # é™ä½éš¨æ©Ÿæ€§
)
```

### 3. éŒ¯èª¤è™•ç†

```python
async def robust_agent_interaction(agent, message):
    try:
        response = await agent.process_message(message)
        return response
    except Exception as e:
        logging.error(f"Agent è™•ç†å¤±æ•—: {e}")
        # å¯¦æ–½é‡è©¦é‚è¼¯
        return await agent.process_message("æŠ±æ­‰ï¼Œè«‹é‡æ–°è¡¨é”æ‚¨çš„å•é¡Œ")
```

### 4. è³‡æºç®¡ç†

```python
# é©ç•¶çš„è³‡æºæ¸…ç†
async def cleanup_agent(agent):
    # æ¸…ç†ç‹€æ…‹
    agent.reset_state()

    # é—œé–‰å·¥å…·é€£æ¥
    if hasattr(agent.tool_manager, 'close'):
        await agent.tool_manager.close()
```

### 5. å®‰å…¨è€ƒé‡

- **è¼¸å…¥é©—è­‰**: é©—è­‰ç”¨æˆ¶è¼¸å…¥çš„å®‰å…¨æ€§
- **å·¥å…·é™åˆ¶**: é™åˆ¶å·¥å…·çš„åŸ·è¡Œæ¬Šé™
- **æ—¥èªŒè¨˜éŒ„**: è¨˜éŒ„é‡è¦æ“ä½œç”¨æ–¼å¯©è¨ˆ

```python
# å®‰å…¨çš„å·¥å…·é…ç½®
safe_tools = ["web_search", "calculator"]  # é¿å…æ–‡ä»¶ç³»çµ±æ“ä½œå·¥å…·
```

---

## ğŸ¯ å¯¦éš›æ‡‰ç”¨å ´æ™¯

ä»¥ä¸‹æ˜¯ä¸€äº›å…·é«”çš„æ‡‰ç”¨å ´æ™¯ï¼Œå±•ç¤ºå¦‚ä½•åœ¨å¯¦éš›å°ˆæ¡ˆä¸­ä½¿ç”¨ Agent Templateã€‚

### å ´æ™¯ 1: ç ”ç©¶å ±å‘Šç”Ÿæˆ

é©ç”¨æ–¼éœ€è¦æ”¶é›†è³‡è¨Šã€åˆ†ææ•¸æ“šä¸¦æ’°å¯«å ±å‘Šçš„å ´æ™¯ã€‚

```python
import asyncio
from agent import AgentFactory, LLM_Provider

async def research_process():
    """åŸ·è¡Œå®Œæ•´çš„ç ”ç©¶å’Œå ±å‘Šç”Ÿæˆæµç¨‹"""
    llm = LLM_Provider(model="gpt-4", provider="openai")
    factory = AgentFactory()

    # å‰µå»ºç ”ç©¶å“¡ Agent
    researcher = factory.create_agent(
        name="ç ”ç©¶å“¡",
        description="å°ˆæ¥­ç ”ç©¶å“¡ï¼Œæ“…é•·è³‡æ–™æ”¶é›†å’Œåˆ†æ",
        system_prompt="""ä½ æ˜¯ä¸€ä½å°ˆæ¥­ç ”ç©¶å“¡ï¼Œå…·å‚™ä»¥ä¸‹èƒ½åŠ›ï¼š
- ç³»çµ±æ€§è³‡è¨Šæœé›†èˆ‡æ•´ç†
- æ‰¹åˆ¤æ€§æ€ç¶­å’Œåˆ†æèƒ½åŠ›
- å¯é è³‡æ–™ä¾†æºé©—è­‰
- çµæ§‹åŒ–å ±å‘Šæ’°å¯«

å·¥ä½œæµç¨‹ï¼š
1. ç†è§£ç ”ç©¶ä¸»é¡Œå’Œéœ€æ±‚
2. åˆ¶å®šç ”ç©¶ç­–ç•¥å’Œæ–¹æ³•
3. æ”¶é›†ç›¸é—œè³‡æ–™å’Œæ•¸æ“š
4. åˆ†ææ•´ç†ä¸¦å¾—å‡ºçµè«–
5. æ’°å¯«è©³ç´°çš„ç ”ç©¶å ±å‘Š

è«‹ä¿æŒå­¸è¡“åš´è¬¹æ€§å’Œå®¢è§€æ€§ã€‚""",
        model=llm.model,
        tools=["web_search", "pdf_reader", "data_analyzer"],
        max_iterations=15,
        temperature=0.3
    )

    # å‰µå»ºå¯«ä½œåŠ©æ‰‹ Agent
    writer = factory.create_agent(
        name="å¯«ä½œåŠ©æ‰‹",
        description="å°ˆæ¥­å…§å®¹å‰µä½œå’Œç·¨è¼¯å°ˆå®¶",
        system_prompt="""ä½ æ˜¯ä¸€ä½å°ˆæ¥­å¯«ä½œåŠ©æ‰‹ï¼Œå°ˆç²¾æ–¼ï¼š
- å…§å®¹çµæ§‹è¦åŠƒå’Œå„ªåŒ–
- èªè¨€è¡¨é”å’Œæ–‡é¢¨èª¿æ•´
- è®€è€…é«”é©—å’Œå¯è®€æ€§æå‡
- å“è³ªæ§åˆ¶å’Œæ ¡å°

å¯«ä½œåŸå‰‡ï¼š
- é‚è¼¯æ¸…æ™°ï¼Œçµæ§‹åˆç†
- èªè¨€æµæš¢ï¼Œè¡¨é”æº–ç¢º
- é‡é»çªå‡ºï¼Œå±¤æ¬¡åˆ†æ˜
- ç¬¦åˆç›®æ¨™è®€è€…éœ€æ±‚

è«‹å‰µä½œå¼•äººå…¥å‹ä¸”å°ˆæ¥­çš„å…§å®¹ã€‚""",
        model=llm.model,
        tools=["grammar_checker", "style_analyzer", "readability_checker"],
        max_iterations=10,
        temperature=0.5
    )

    # åˆå§‹åŒ– Agents
    await researcher.initialize()
    await writer.initialize()

    # åŸ·è¡Œç ”ç©¶å’Œå¯«ä½œæµç¨‹
    research_topic = "2024å¹´äººå·¥æ™ºæ…§åœ¨é†«ç™‚é ˜åŸŸçš„æœ€æ–°ç™¼å±•"

    print("ğŸ” é–‹å§‹ç ”ç©¶éšæ®µ...")
    research_data = await researcher.process_message(
        f"è«‹æ·±å…¥ç ”ç©¶ã€Œ{research_topic}ã€ï¼ŒåŒ…æ‹¬ï¼š\n"
        "1. æœ€æ–°æŠ€è¡“ç™¼å±•è¶¨å‹¢\n"
        "2. å¯¦éš›æ‡‰ç”¨æ¡ˆä¾‹åˆ†æ\n"
        "3. é¢è‡¨çš„æŒ‘æˆ°å’Œé™åˆ¶\n"
        "4. æœªä¾†ç™¼å±•é æ¸¬\n\n"
        "è«‹æä¾›è©³ç´°çš„ç ”ç©¶å ±å‘Šï¼ŒåŒ…å«æ•¸æ“šæ”¯æ’å’Œå¯é ä¾†æºã€‚"
    )

    print("âœï¸ é–‹å§‹å¯«ä½œéšæ®µ...")
    final_report = await writer.process_message(
        f"åŸºæ–¼ä»¥ä¸‹ç ”ç©¶è³‡æ–™ï¼Œæ’°å¯«ä¸€ä»½å°ˆæ¥­ä¸”æ˜“è®€çš„å ±å‘Šï¼š\n\n{research_data}\n\n"
        "è¦æ±‚ï¼š\n"
        "- çµæ§‹æ¸…æ™°ï¼ŒåŒ…å«æ‘˜è¦ã€æ­£æ–‡ã€çµè«–\n"
        "- èªè¨€å°ˆæ¥­ä½†æ˜“æ‡‚\n"
        "- é‡é»çªå‡ºï¼Œé‚è¼¯é †æš¢\n"
        "- é©åˆéå°ˆæ¥­è®€è€…é–±è®€"
    )

    return final_report

# ä½¿ç”¨ç¯„ä¾‹
async def main():
    report = await research_process()
    print("ğŸ“„ ç ”ç©¶å ±å‘Šç”Ÿæˆå®Œæˆï¼š")
    print(report)

if __name__ == "__main__":
    asyncio.run(main())
```

### å ´æ™¯ 2: ç¨‹å¼é–‹ç™¼è¼”åŠ©

é©ç”¨æ–¼ä»£ç¢¼é–‹ç™¼ã€å¯©æŸ¥å’Œå„ªåŒ–çš„å ´æ™¯ã€‚

```python
import asyncio
from agent import AgentFactory, LLM_Provider

async def coding_process():
    """åŸ·è¡Œç¨‹å¼é–‹ç™¼å’Œä»£ç¢¼å¯©æŸ¥æµç¨‹"""
    llm = LLM_Provider(model="gpt-4", provider="openai")
    factory = AgentFactory()

    # å‰µå»ºç¨‹å¼è¨­è¨ˆå¸« Agent
    coder = factory.create_agent(
        name="ç¨‹å¼è¨­è¨ˆå¸«",
        description="è³‡æ·±è»Ÿé«”å·¥ç¨‹å¸«ï¼Œå°ˆæ³¨æ–¼é«˜å“è³ªä»£ç¢¼é–‹ç™¼",
        system_prompt="""ä½ æ˜¯ä¸€ä½è³‡æ·±è»Ÿé«”å·¥ç¨‹å¸«ï¼Œå…·å‚™ä»¥ä¸‹æŠ€èƒ½ï¼š
- å¤šèªè¨€ç¨‹å¼è¨­è¨ˆç¶“é©—
- è»Ÿé«”æ¶æ§‹è¨­è¨ˆèƒ½åŠ›
- æœ€ä½³å¯¦è¸å’Œè¨­è¨ˆæ¨¡å¼æ‡‰ç”¨
- æ€§èƒ½å„ªåŒ–å’Œå®‰å…¨è€ƒé‡

é–‹ç™¼åŸå‰‡ï¼š
- ä»£ç¢¼å¯è®€æ€§å’Œå¯ç¶­è­·æ€§
- éµå¾ªç·¨ç¨‹è¦ç¯„å’Œæœ€ä½³å¯¦è¸
- è€ƒæ…®æ€§èƒ½å’Œå®‰å…¨æ€§
- æ’°å¯«æ¸…æ™°çš„è¨»é‡‹å’Œæ–‡æª”

è«‹ç¢ºä¿ä»£ç¢¼å“è³ªå’Œå°ˆæ¥­æ€§ã€‚""",
        model=llm.model,
        tools=["code_executor", "linter", "formatter"],
        max_iterations=12,
        temperature=0.2
    )

    # å‰µå»ºä»£ç¢¼å¯©æŸ¥å“¡ Agent
    reviewer = factory.create_agent(
        name="å¯©æŸ¥å“¡",
        description="ä»£ç¢¼å“è³ªå°ˆå®¶ï¼Œå°ˆæ³¨æ–¼ä»£ç¢¼å¯©æŸ¥å’Œå„ªåŒ–å»ºè­°",
        system_prompt="""ä½ æ˜¯ä¸€ä½ä»£ç¢¼å¯©æŸ¥å°ˆå®¶ï¼Œå°ˆæ³¨æ–¼ï¼š
- ä»£ç¢¼å“è³ªè©•ä¼°å’Œæ”¹é€²å»ºè­°
- å®‰å…¨æ¼æ´è­˜åˆ¥å’Œä¿®å¾©
- æ€§èƒ½ç“¶é ¸åˆ†æå’Œå„ªåŒ–
- æœ€ä½³å¯¦è¸åˆè¦æ€§æª¢æŸ¥

å¯©æŸ¥é‡é»ï¼š
- ä»£ç¢¼é‚è¼¯æ­£ç¢ºæ€§
- å®‰å…¨æ€§å’Œç©©å®šæ€§
- æ€§èƒ½å’Œæ•ˆç‡
- å¯è®€æ€§å’Œå¯ç¶­è­·æ€§
- æ¸¬è©¦è¦†è“‹ç‡

è«‹æä¾›å…·é«”ä¸”å¯æ“ä½œçš„æ”¹é€²å»ºè­°ã€‚""",
        model=llm.model,
        tools=["security_scanner", "performance_analyzer", "test_generator"],
        max_iterations=8,
        temperature=0.1
    )

    # åˆå§‹åŒ– Agents
    await coder.initialize()
    await reviewer.initialize()

    # é–‹ç™¼å’Œå¯©æŸ¥æµç¨‹
    requirement = """
    éœ€æ±‚ï¼šå¯¦ç¾ä¸€å€‹é«˜æ•ˆçš„åˆ†æ•£å¼å¿«å–ç³»çµ±

    åŠŸèƒ½è¦æ±‚ï¼š
    1. æ”¯æ´å¤šç¨®æ•¸æ“šé¡å‹å­˜å„²
    2. å…·å‚™è‡ªå‹•éæœŸæ©Ÿåˆ¶
    3. æ”¯æ´é›†ç¾¤éƒ¨ç½²
    4. æä¾›ç›£æ§å’Œçµ±è¨ˆåŠŸèƒ½
    5. ç¢ºä¿æ•¸æ“šä¸€è‡´æ€§

    æŠ€è¡“è¦æ±‚ï¼š
    - ä½¿ç”¨ Python å¯¦ç¾
    - æ”¯æ´ Redis ä½œç‚ºå¾Œç«¯
    - åŒ…å«å®Œæ•´çš„éŒ¯èª¤è™•ç†
    - æä¾›å–®å…ƒæ¸¬è©¦
    """

    print("ğŸ‘¨â€ğŸ’» é–‹å§‹é–‹ç™¼éšæ®µ...")
    code_result = await coder.process_message(
        f"è«‹æ ¹æ“šä»¥ä¸‹éœ€æ±‚é–‹ç™¼ä»£ç¢¼ï¼š\n{requirement}\n\n"
        "è«‹æä¾›ï¼š\n"
        "1. å®Œæ•´çš„é¡è¨­è¨ˆå’Œå¯¦ç¾\n"
        "2. è©³ç´°çš„è¨»é‡‹èªªæ˜\n"
        "3. ä½¿ç”¨ç¯„ä¾‹\n"
        "4. åŸºæœ¬çš„å–®å…ƒæ¸¬è©¦"
    )

    print("ğŸ” é–‹å§‹å¯©æŸ¥éšæ®µ...")
    review_result = await reviewer.process_message(
        f"è«‹å¯©æŸ¥ä»¥ä¸‹ä»£ç¢¼ï¼Œä¸¦æä¾›æ”¹é€²å»ºè­°ï¼š\n\n{code_result}\n\n"
        "å¯©æŸ¥é‡é»ï¼š\n"
        "- ä»£ç¢¼æ¶æ§‹å’Œè¨­è¨ˆæ¨¡å¼\n"
        "- æ€§èƒ½å’Œå®‰å…¨æ€§\n"
        "- éŒ¯èª¤è™•ç†å’Œé‚Šç•Œæƒ…æ³\n"
        "- æ¸¬è©¦å®Œæ•´æ€§\n"
        "- æ–‡æª”å’Œè¨»é‡‹å“è³ª"
    )

    return {
        "original_code": code_result,
        "review_feedback": review_result
    }

# ä½¿ç”¨ç¯„ä¾‹
async def main():
    result = await coding_process()
    print("ğŸ’» é–‹ç™¼çµæœï¼š")
    print(result["original_code"])
    print("\nğŸ” å¯©æŸ¥æ„è¦‹ï¼š")
    print(result["review_feedback"])

if __name__ == "__main__":
    asyncio.run(main())
```

### å ´æ™¯ 3: å•†æ¥­è«®è©¢æµç¨‹

é©ç”¨æ–¼å•†æ¥­åˆ†æã€ç­–ç•¥åˆ¶å®šå’Œå•é¡Œè§£æ±ºçš„å ´æ™¯ã€‚

```python
import asyncio
from agent import AgentFactory, LLM_Provider

async def business_consulting():
    """åŸ·è¡Œå•†æ¥­è«®è©¢å’Œç­–ç•¥åˆ†æ"""
    llm = LLM_Provider(model="gpt-4", provider="openai")
    factory = AgentFactory()

    # å‰µå»ºå•†æ¥­é¡§å• Agent
    consultant = factory.create_agent(
        name="å•†æ¥­é¡§å•",
        description="è³‡æ·±å•†æ¥­ç­–ç•¥é¡§å•ï¼Œå°ˆç²¾ä¼æ¥­è«®è©¢å’Œæˆ°ç•¥è¦åŠƒ",
        system_prompt="""ä½ æ˜¯ä¸€ä½è³‡æ·±å•†æ¥­é¡§å•ï¼Œå…·å‚™ä»¥ä¸‹å°ˆæ¥­èƒ½åŠ›ï¼š
- å•†æ¥­ç­–ç•¥åˆ¶å®šå’ŒåŸ·è¡Œ
- å¸‚å ´åˆ†æå’Œç«¶çˆ­æƒ…å ±
- çµ„ç¹”è¨ºæ–·å’Œå„ªåŒ–
- é¢¨éšªè©•ä¼°å’Œç®¡ç†
- è²¡å‹™åˆ†æå’Œé æ¸¬

è«®è©¢æ–¹æ³•ï¼š
- çµæ§‹åŒ–å•é¡Œåˆ†æ
- æ•¸æ“šé©…å‹•çš„æ±ºç­–æ”¯æŒ
- å¯¦ç”¨ä¸”å¯åŸ·è¡Œçš„å»ºè­°
- é¢¨éšªå’Œæ©Ÿæœƒä¸¦é‡
- çŸ­æœŸå’Œé•·æœŸå¹³è¡¡è€ƒé‡

è«‹æä¾›å°ˆæ¥­ã€å¯¦ç”¨ä¸”å…·æœ‰æ“ä½œæ€§çš„å•†æ¥­å»ºè­°ã€‚""",
        model=llm.model,
        tools=["market_analyzer", "financial_calculator", "risk_assessor"],
        max_iterations=15,
        temperature=0.3
    )

    # å‰µå»ºæ•¸æ“šåˆ†æå¸« Agent
    analyst = factory.create_agent(
        name="æ•¸æ“šåˆ†æå¸«",
        description="è³‡æ·±æ•¸æ“šåˆ†æå¸«ï¼Œå°ˆç²¾å•†æ¥­æ•¸æ“šåˆ†æå’Œæ´å¯Ÿç™¼ç¾",
        system_prompt="""ä½ æ˜¯ä¸€ä½è³‡æ·±æ•¸æ“šåˆ†æå¸«ï¼Œå°ˆç²¾æ–¼ï¼š
- å•†æ¥­æ•¸æ“šæ”¶é›†å’Œæ¸…ç†
- çµ±è¨ˆåˆ†æå’Œè¶¨å‹¢è­˜åˆ¥
- æ•¸æ“šè¦–è¦ºåŒ–å’Œå ±å‘Š
- é æ¸¬æ¨¡å‹å»ºç«‹
- æ¥­å‹™æŒ‡æ¨™ç›£æ§

åˆ†æåŸå‰‡ï¼š
- æ•¸æ“šæº–ç¢ºæ€§å’Œå¯é æ€§
- çµ±è¨ˆæ–¹æ³•çš„æ­£ç¢ºæ‡‰ç”¨
- æ¸…æ™°çš„è¦–è¦ºåŒ–å‘ˆç¾
- å¯¦ç”¨çš„å•†æ¥­æ´å¯Ÿ
- å¯æ“ä½œçš„å»ºè­°

è«‹æä¾›åŸºæ–¼æ•¸æ“šçš„å®¢è§€åˆ†æå’Œæ´å¯Ÿã€‚""",
        model=llm.model,
        tools=["data_processor", "chart_generator", "trend_analyzer"],
        max_iterations=12,
        temperature=0.2
    )

    # åˆå§‹åŒ– Agents
    await consultant.initialize()
    await analyst.initialize()

    # å•†æ¥­è«®è©¢æ¡ˆä¾‹
    business_challenge = """
    å…¬å¸èƒŒæ™¯ï¼šä¸€å®¶ä¸­å‹é›»å•†å¹³å°

    é¢è‡¨å•é¡Œï¼š
    - å®¢æˆ¶ç•™å­˜ç‡æŒçºŒä¸‹é™ï¼ˆå¾ 65% é™è‡³ 45%ï¼‰
    - ç²å®¢æˆæœ¬ä¸æ–·ä¸Šå‡ï¼ˆå¢é•· 40%ï¼‰
    - ç«¶çˆ­å°æ‰‹æ¨å‡ºæ›´å„ªæƒ çš„æœå‹™
    - å®¢æˆ¶æ»¿æ„åº¦è©•åˆ†ä¸‹æ»‘

    ç¾æœ‰æ•¸æ“šï¼š
    - æœˆæ´»èºç”¨æˆ¶ï¼š100è¬
    - å¹³å‡è¨‚å–®é‡‘é¡ï¼š$85
    - å®¢æœå›æ‡‰æ™‚é–“ï¼š24å°æ™‚
    - é€€è²¨ç‡ï¼š12%
    """

    print("ğŸ“Š é–‹å§‹æ•¸æ“šåˆ†æ...")
    data_analysis = await analyst.process_message(
        f"è«‹åˆ†æä»¥ä¸‹å•†æ¥­å•é¡Œçš„æ•¸æ“šé¢å‘ï¼š\n{business_challenge}\n\n"
        "è«‹æä¾›ï¼š\n"
        "1. å•é¡Œçš„æ•¸æ“šåˆ†ææ¡†æ¶\n"
        "2. é—œéµæŒ‡æ¨™çš„è¶¨å‹¢åˆ†æ\n"
        "3. å¯èƒ½çš„æ ¹æœ¬åŸå› è­˜åˆ¥\n"
        "4. å»ºè­°æ”¶é›†çš„é¡å¤–æ•¸æ“š\n"
        "5. é‡åŒ–è©•ä¼°æ–¹æ³•"
    )

    print("ğŸ’¼ é–‹å§‹ç­–ç•¥è«®è©¢...")
    strategic_advice = await consultant.process_message(
        f"åŸºæ–¼ä»¥ä¸‹å•†æ¥­æŒ‘æˆ°å’Œæ•¸æ“šåˆ†æï¼Œè«‹æä¾›ç­–ç•¥å»ºè­°ï¼š\n\n"
        f"å•†æ¥­æŒ‘æˆ°ï¼š\n{business_challenge}\n\n"
        f"æ•¸æ“šåˆ†æï¼š\n{data_analysis}\n\n"
        "è«‹æä¾›ï¼š\n"
        "1. å•é¡Œè¨ºæ–·å’Œæ ¹æœ¬åŸå› \n"
        "2. çŸ­æœŸå’Œé•·æœŸè§£æ±ºæ–¹æ¡ˆ\n"
        "3. å¯¦æ–½å„ªå…ˆé †åºå’Œæ™‚é–“è¦åŠƒ\n"
        "4. é æœŸæ•ˆæœå’Œé¢¨éšªè©•ä¼°\n"
        "5. æˆåŠŸæŒ‡æ¨™å’Œç›£æ§æ–¹å¼"
    )

    return {
        "data_analysis": data_analysis,
        "strategic_advice": strategic_advice
    }

# ä½¿ç”¨ç¯„ä¾‹
async def main():
    result = await business_consulting()
    print("ğŸ“Š æ•¸æ“šåˆ†æå ±å‘Šï¼š")
    print(result["data_analysis"])
    print("\nğŸ’¼ ç­–ç•¥è«®è©¢å»ºè­°ï¼š")
    print(result["strategic_advice"])

if __name__ == "__main__":
    asyncio.run(main())
```

### å ´æ™¯ 4: å…§å®¹å‰µä½œç®¡ç·š

é©ç”¨æ–¼å¤šåª’é«”å…§å®¹å‰µä½œå’Œç·¨è¼¯çš„å ´æ™¯ã€‚

```python
import asyncio
from agent import AgentFactory, LLM_Provider

async def content_creation_pipeline():
    """åŸ·è¡Œå®Œæ•´çš„å…§å®¹å‰µä½œæµç¨‹"""
    llm = LLM_Provider(model="gpt-4", provider="openai")
    factory = AgentFactory()

    # å‰µå»ºå…§å®¹ç­–åŠƒå¸«
    planner = factory.create_agent(
        name="å…§å®¹ç­–åŠƒå¸«",
        description="å…§å®¹ç­–ç•¥å°ˆå®¶ï¼Œè² è²¬å…§å®¹è¦åŠƒå’Œä¸»é¡Œè¨­è¨ˆ",
        system_prompt="""ä½ æ˜¯å…§å®¹ç­–ç•¥å°ˆå®¶ï¼Œå°ˆç²¾æ–¼ï¼š
- ç›®æ¨™å—çœ¾åˆ†æå’Œå…§å®¹å®šä½
- å…§å®¹ä¸»é¡Œè¦åŠƒå’Œå‰µæ„ç™¼æƒ³
- å¤šå¹³å°å…§å®¹ç­–ç•¥åˆ¶å®š
- å…§å®¹è¡ŒéŠ·æ•ˆæœè©•ä¼°

ç­–åŠƒåŸå‰‡ï¼š
- æ·±åº¦ç†è§£ç›®æ¨™å—çœ¾éœ€æ±‚
- å‰µæ–°ä¸”æœ‰å¸å¼•åŠ›çš„å…§å®¹ä¸»é¡Œ
- è·¨å¹³å°å…§å®¹é©é…ç­–ç•¥
- å¯è¡¡é‡çš„å…§å®¹æ•ˆæœç›®æ¨™

è«‹æä¾›å…·æœ‰å‰µæ„ä¸”å¯¦ç”¨çš„å…§å®¹ç­–ç•¥ã€‚""",
        model=llm.model,
        tools=["audience_analyzer", "trend_monitor"],
        max_iterations=10,
        temperature=0.7
    )

    # å‰µå»ºå…§å®¹æ’°å¯«è€…
    writer = factory.create_agent(
        name="å…§å®¹æ’°å¯«è€…",
        description="å°ˆæ¥­å…§å®¹å‰µä½œè€…ï¼Œæ“…é•·å¤šç¨®å½¢å¼çš„å…§å®¹å‰µä½œ",
        system_prompt="""ä½ æ˜¯å°ˆæ¥­å…§å®¹å‰µä½œè€…ï¼Œå…·å‚™ï¼š
- å¤šç¨®æ–‡é«”å’Œæ ¼å¼çš„å¯«ä½œèƒ½åŠ›
- ä¸åŒå¹³å°å…§å®¹ç‰¹æ€§ç†è§£
- SEO å’Œé—œéµå­—å„ªåŒ–æŠ€å·§
- è®€è€…åƒèˆ‡åº¦æå‡ç­–ç•¥

å‰µä½œåŸå‰‡ï¼š
- å…§å®¹åŸå‰µæ€§å’Œå“è³ª
- é©åˆç›®æ¨™å¹³å°çš„é¢¨æ ¼
- è®€è€…å‹å–„å’Œæ˜“è®€æ€§
- æœ‰æ•ˆçš„ CTA å’Œäº’å‹•è¨­è¨ˆ

è«‹å‰µä½œå¼•äººå…¥å‹ä¸”æœ‰åƒ¹å€¼çš„å…§å®¹ã€‚""",
        model=llm.model,
        tools=["seo_optimizer", "readability_checker"],
        max_iterations=12,
        temperature=0.6
    )

    # åˆå§‹åŒ– Agents
    await planner.initialize()
    await writer.initialize()

    # å…§å®¹å‰µä½œéœ€æ±‚
    content_brief = """
    å…§å®¹ç›®æ¨™ï¼šæ¨å»£ä¸€æ¬¾æ–°çš„å¥èº« App
    ç›®æ¨™å—çœ¾ï¼š25-40 æ­²æ³¨é‡å¥åº·çš„ä¸Šç­æ—
    å¹³å°ï¼šéƒ¨è½æ ¼ã€ç¤¾ç¾¤åª’é«”ã€é›»å­å ±

    App ç‰¹è‰²ï¼š
    - AI å€‹äººåŒ–è¨“ç·´è¨ˆç•«
    - å±…å®¶å¥èº«èª²ç¨‹
    - ç‡Ÿé¤Šå»ºè­°å’Œè¿½è¹¤
    - ç¤¾ç¾¤æŒ‘æˆ°åŠŸèƒ½

    å…§å®¹éœ€æ±‚ï¼š
    - 3 ç¯‡éƒ¨è½æ ¼æ–‡ç« 
    - 10 å‰‡ç¤¾ç¾¤åª’é«”è²¼æ–‡
    - 1 ä»½é›»å­å ±å…§å®¹
    """

    print("ğŸ“‹ é–‹å§‹å…§å®¹ç­–åŠƒ...")
    content_strategy = await planner.process_message(
        f"è«‹ç‚ºä»¥ä¸‹å…§å®¹éœ€æ±‚åˆ¶å®šç­–ç•¥ï¼š\n{content_brief}\n\n"
        "è«‹æä¾›ï¼š\n"
        "1. ç›®æ¨™å—çœ¾è©³ç´°åˆ†æ\n"
        "2. å…§å®¹ä¸»é¡Œå’Œè§’åº¦å»ºè­°\n"
        "3. å„å¹³å°å…§å®¹ç­–ç•¥\n"
        "4. å…§å®¹æ—¥ç¨‹å®‰æ’\n"
        "5. æˆæ•ˆè©•ä¼°æŒ‡æ¨™"
    )

    print("âœï¸ é–‹å§‹å…§å®¹å‰µä½œ...")
    content_creation = await writer.process_message(
        f"åŸºæ–¼ä»¥ä¸‹ç­–ç•¥ï¼Œå‰µä½œå…·é«”å…§å®¹ï¼š\n\n{content_strategy}\n\n"
        "è«‹æä¾›ï¼š\n"
        "1. 3 ç¯‡éƒ¨è½æ ¼æ–‡ç« æ¨™é¡Œå’Œå¤§ç¶±\n"
        "2. 10 å‰‡ç¤¾ç¾¤åª’é«”è²¼æ–‡å…§å®¹\n"
        "3. 1 ä»½é›»å­å ±å®Œæ•´å…§å®¹\n"
        "4. SEO é—œéµå­—å»ºè­°\n"
        "5. è¦–è¦ºå…§å®¹å»ºè­°"
    )

    return {
        "strategy": content_strategy,
        "content": content_creation
    }

# ä½¿ç”¨ç¯„ä¾‹
async def main():
    result = await content_creation_pipeline()
    print("ğŸ“‹ å…§å®¹ç­–ç•¥ï¼š")
    print(result["strategy"])
    print("\nâœï¸ å‰µä½œå…§å®¹ï¼š")
    print(result["content"])

if __name__ == "__main__":
    asyncio.run(main())
```

---

## ğŸ”— ç›¸é—œè³‡æº

- [LangChain æ–‡æª”](https://python.langchain.com/)
- [LangGraph æŒ‡å—](https://langchain-ai.github.io/langgraph/)
- [MCP å”è­°è¦ç¯„](https://modelcontextprotocol.io/)
- [Ollama ä½¿ç”¨æŒ‡å—](https://ollama.com/)
